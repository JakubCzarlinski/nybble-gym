{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pybullet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vedal987/nybble-gym/blob/main/train-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Stable Baselines3 - PyBullet: Normalizing Features and Reward\n",
        "\n",
        "Github Repo: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n",
        "\n",
        "Pybullet source code: https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "source": [
        "!pip install stable-baselines3[extra] pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Import policy, RL agent, Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "import os \n",
        "\n",
        "import pybullet_envs\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecNormalize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c8VHsiXC7dL"
      },
      "source": [
        "## Create and wrap the environment with `VecNormalize`\n",
        "\n",
        "Normalizing input features may be essential to successful training of an RL agent (by default, images are scaled but not other types of input), for instance when training on [PyBullet](https://github.com/bulletphysics/bullet3/) environments. For that, a wrapper exists and will compute a running average and standard deviation of input features (it can do the same for rewards).\n",
        "\n",
        "More information about `VecNormalize`:\n",
        "- [Documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#stable_baselines3.common.vec_env.VecNormalize)\n",
        "- [Discussion](https://github.com/hill-a/stable-baselines/issues/698)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmxIq5UeC3Nj"
      },
      "source": [
        "env = make_vec_env(\"HalfCheetahBulletEnv-v0\", n_envs=1)\n",
        "\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxUMGsl5mabF"
      },
      "source": [
        "### Train the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQmsSZUHKNRG"
      },
      "source": [
        "model = PPO('MlpPolicy', env, verbose=1)\n",
        "model.learn(total_timesteps=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZYBIVoLmcR4"
      },
      "source": [
        "###Â Save the agent and the normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpMDXP0vmezv"
      },
      "source": [
        "# Don't forget to save the VecNormalize statistics when saving the agent\n",
        "log_dir = \"/tmp/\"\n",
        "model.save(log_dir + \"ppo_halfcheetah\")\n",
        "stats_path = os.path.join(log_dir, \"vec_normalize.pkl\")\n",
        "env.save(stats_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eezphIrRmr-Y"
      },
      "source": [
        "### Test model: load the saved agent and normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQT1k7lWmmTL"
      },
      "source": [
        "# Load the agent\n",
        "model = PPO.load(log_dir + \"ppo_halfcheetah\")\n",
        "\n",
        "# Load the saved statistics\n",
        "env = make_vec_env(\"HalfCheetahBulletEnv-v0\", n_envs=1)\n",
        "env = VecNormalize.load(stats_path, env)\n",
        "#  do not update them at test time\n",
        "env.training = False\n",
        "# reward normalization is not needed at test time\n",
        "env.norm_reward = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voCxMxfYm2yc"
      },
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zC2IMJUm3Kw"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tHTCBIynFhd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}